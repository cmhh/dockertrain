{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    div.container {\n",
    "      max-width: 800px!important;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "# SparkML Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36minit\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init: Unit = {\n",
    "  import ammonite.ops._\n",
    "  val jars = ls! root/'opt/'spark/'jars |? (_.ext == \"jar\")\n",
    "  jars.foreach(interp.load.cp(_))   \n",
    "}\n",
    "\n",
    "init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@6e04003e"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "  .builder\n",
    "  .config(\"hive.metastore.uris\",\"thrift://localhost:9083\") \n",
    "  .config(\"spark.sql.warehouse.dir\", \"/data/hive/warehouse\")\n",
    "  .master(\"local[*]\")\n",
    "  .appName(\"Spark SQL Basics\")\n",
    "  .enableHiveSupport()\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@2e0bcb77"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-------+\n",
      "|sepalLength|sepalWidth|petalLength|petalWidth|Species|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "|        5.1|       3.5|        1.4|       0.2| setosa|\n",
      "|        4.9|       3.0|        1.4|       0.2| setosa|\n",
      "|        4.7|       3.2|        1.3|       0.2| setosa|\n",
      "|        4.6|       3.1|        1.5|       0.2| setosa|\n",
      "|        5.0|       3.6|        1.4|       0.2| setosa|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36miris\u001b[39m: \u001b[32mDataFrame\u001b[39m = [sepalLength: double, sepalWidth: double ... 3 more fields]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iris = spark\n",
    "  .read\n",
    "  .format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"delimiter\", \",\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"file:///data/csv/iris.csv\")\n",
    "  .withColumnRenamed(\"Sepal.Length\", \"sepalLength\")\n",
    "  .withColumnRenamed(\"Sepal.Width\", \"sepalWidth\")\n",
    "  .withColumnRenamed(\"Petal.Length\", \"petalLength\")\n",
    "  .withColumnRenamed(\"Petal.Width\", \"petalWidth\")\n",
    "  \n",
    "iris.limit(5).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrain\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [sepalLength: double, sepalWidth: double ... 3 more fields]\n",
       "\u001b[36mtest\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [sepalLength: double, sepalWidth: double ... 3 more fields]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val List(train, test) = iris.randomSplit(Array(0.8, 0.2), 1).toList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres11\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m118L\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres12\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m32L\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-------+-----+\n",
      "|sepalLength|sepalWidth|petalLength|petalWidth|Species|label|\n",
      "+-----------+----------+-----------+----------+-------+-----+\n",
      "|        5.1|       3.5|        1.4|       0.2| setosa|  2.0|\n",
      "|        4.9|       3.0|        1.4|       0.2| setosa|  2.0|\n",
      "|        4.7|       3.2|        1.3|       0.2| setosa|  2.0|\n",
      "|        4.6|       3.1|        1.5|       0.2| setosa|  2.0|\n",
      "|        5.0|       3.6|        1.4|       0.2| setosa|  2.0|\n",
      "+-----------+----------+-----------+----------+-------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mlabelIndexer\u001b[39m: \u001b[32mStringIndexer\u001b[39m = strIdx_b26f47000635\n",
       "\u001b[36md1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [sepalLength: double, sepalWidth: double ... 4 more fields]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelIndexer = new StringIndexer()\n",
    "  .setInputCol(\"Species\")\n",
    "  .setOutputCol(\"label\")\n",
    "\n",
    "val d1 = labelIndexer.fit(iris).transform(iris)\n",
    "\n",
    "d1.limit(5).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-------+-----+-----------------+\n",
      "|sepalLength|sepalWidth|petalLength|petalWidth|Species|label|         features|\n",
      "+-----------+----------+-----------+----------+-------+-----+-----------------+\n",
      "|        5.1|       3.5|        1.4|       0.2| setosa|  2.0|[5.1,3.5,1.4,0.2]|\n",
      "|        4.9|       3.0|        1.4|       0.2| setosa|  2.0|[4.9,3.0,1.4,0.2]|\n",
      "|        4.7|       3.2|        1.3|       0.2| setosa|  2.0|[4.7,3.2,1.3,0.2]|\n",
      "|        4.6|       3.1|        1.5|       0.2| setosa|  2.0|[4.6,3.1,1.5,0.2]|\n",
      "|        5.0|       3.6|        1.4|       0.2| setosa|  2.0|[5.0,3.6,1.4,0.2]|\n",
      "+-----------+----------+-----------+----------+-------+-----+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfeatureIndexer\u001b[39m: \u001b[32mVectorAssembler\u001b[39m = vecAssembler_d4a6aa201603\n",
       "\u001b[36md2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [sepalLength: double, sepalWidth: double ... 5 more fields]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featureIndexer = new VectorAssembler()\n",
    "  .setInputCols(Array(\"sepalLength\", \"sepalWidth\", \"petalLength\", \"petalWidth\"))\n",
    "  .setOutputCol(\"features\")\n",
    "  \n",
    "val d2 = featureIndexer.transform(d1)\n",
    "\n",
    "d2.limit(5).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/07/25 21:14:01 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "19/07/25 21:14:01 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.7344130579787816\n",
      "Cluster Centers: \n",
      "[5.88360655737705,2.7409836065573776,4.388524590163936,1.4344262295081969]\n",
      "[6.853846153846153,3.0769230769230766,5.715384615384615,2.053846153846153]\n",
      "[5.005999999999999,3.428000000000001,1.4620000000000002,0.2459999999999999]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mkmeans\u001b[39m: \u001b[32mKMeans\u001b[39m = kmeans_44e7f4b624f5\n",
       "\u001b[36mmodel\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mclustering\u001b[39m.\u001b[32mKMeansModel\u001b[39m = kmeans_44e7f4b624f5\n",
       "\u001b[36mpredictions\u001b[39m: \u001b[32mDataFrame\u001b[39m = [sepalLength: double, sepalWidth: double ... 6 more fields]\n",
       "\u001b[36mevaluator\u001b[39m: \u001b[32mClusteringEvaluator\u001b[39m = cluEval_d58edc4951d5\n",
       "\u001b[36msilhouette\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.7344130579787816\u001b[39m"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler, IndexToString}\n",
    "import org.apache.spark.ml.clustering.KMeans\n",
    "import org.apache.spark.ml.evaluation.ClusteringEvaluator\n",
    "\n",
    "val kmeans = new KMeans().setK(3).setSeed(1L)\n",
    "val model = kmeans.fit(d2)\n",
    "\n",
    "val predictions = model.transform(d2)\n",
    "\n",
    "val evaluator = new ClusteringEvaluator()\n",
    "val silhouette = evaluator.evaluate(predictions)\n",
    "println(s\"Silhouette with squared euclidean distance = $silhouette\")\n",
    "\n",
    "println(\"Cluster Centers: \")\n",
    "model.clusterCenters.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.11)",
   "language": "scala",
   "name": "scala211"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
