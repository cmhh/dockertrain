{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    div.container {\n",
    "      max-width: 800px!important;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "# Spark SQL Basics\n",
    "\n",
    "Spark SQL provides the means for working with structured data within Apache Spark.  Structured data is represented by the `DataFrame` abstraction (which is a type alias for `Dataset[Row]`), and we can act on them using familiar-looking SQL queries, or else the `DataFrame` API.  In this lesson, we cover `DataFrame` basics, including:\n",
    "\n",
    "* creating `DataFrame`s in code\n",
    "* creating `DataFrame`s from external sources (CSV, parquet, hive, PostgreSQL, etc.)\n",
    "* manipulating and summarising `DataFrame`s using both SQL and the `DataFrame` API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "This workbook makes use of the [Almond Scala kernel for Jupyter](https://almond.sh/).  To use Spark, we have to first add a few libraries to the classpath, which we can do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36minit\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init: Unit = {\n",
    "  import ammonite.ops._\n",
    "  val jars = ls! root/'opt/'spark/'jars |? (_.ext == \"jar\")\n",
    "  jars.foreach(interp.load.cp(_))   \n",
    "}\n",
    "\n",
    "init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is also pretty verbose with respect to logging, so it can be useful to change the logging policy to de-clutter our outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just to get it out of the way up front, we import a number of objects that we need throughout the rest of the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, sometimes a code block will produce a large amount of output, some of which is unimportant, and so obfuscatory.  To hide this, we sometimes wrap things in an object like so:\n",
    "\n",
    "```scala\n",
    "object foo {\n",
    "  val x = 1\n",
    "  val y = 2\n",
    "}\n",
    "\n",
    "x + y\n",
    "```\n",
    "\n",
    "The object `foo` serves no functional purpose here other than to hide the interpreter output that results from the assignment of `x` and `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a `SparkSession`\n",
    "\n",
    "As of Spark 2.x, the usual method of interacting with Spark is by creating `SparkSession` to function as a single entrypoing.  In our case, we do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@45be1e88"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "  .builder\n",
    "  .config(\"hive.metastore.uris\",\"thrift://localhost:9083\") \n",
    "  .config(\"spark.sql.warehouse.dir\", \"/data/hive/warehouse\")\n",
    "  .master(\"local[*]\")\n",
    "  .appName(\"Spark SQL Basics\")\n",
    "  .enableHiveSupport()\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we explicitly configure our session to use Hive by setting values for `hive.metastore.uris` and `spark.sql.warehouse.dir`.  It is common for this to work automatically by configuration (via the file `hive-site.xml`), but this does not appear to the case in this context.  We also tell Spark to work in pseudo-distributed mode by setting `master` to `local[*]`.  There are various other configurations possible, but that's out of scope here.  See [Configuration - Spark 2.4.3 Documentation](https://spark.apache.org/docs/latest/configuration.html) for details.\n",
    "\n",
    "When working with Spark SQL, it is very common to use the object `spark.sparkContext`.  So for convenience, we also assign this to a variable, commonly `sc`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@457deaa1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.b. that we decreased the amount of debugging already in above.  We can also do this via the `SparkSession` object by running `sc.setLogLevel(\"ERROR\")`, but then we'd still be subjected to the logging that occurs as a result of creating the `SparkSession` itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Dataset (`RDD`)\n",
    "\n",
    "As noted, `DataFrame` is the central data abstraction when working with structured data.  However, these build on an earlier abstraction called Resilient Distributed Datasets (`RDD`), and one will still have occasion to use these.  An `RDD` is essentially just a normal Scala collection that's been parallelised for use with Spark.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mbeatles\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m\"John\"\u001b[39m, \u001b[32m\"Paul\"\u001b[39m, \u001b[32m\"Ringo\"\u001b[39m, \u001b[32m\"George\"\u001b[39m)\n",
       "\u001b[36mdistributedBeatles\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[0] at parallelize at cmd5.sc:2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val beatles = Seq(\"John\", \"Paul\", \"Ringo\", \"George\")\n",
    "val distributedBeatles = spark.sparkContext.parallelize(beatles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can treat the resulting `RDD` in much the same way as the original collection, but the `RDD` will be worked on in parallel.  This means that when iterating over an `RDD` the order we will process entries will be unstable.  And because of this, certain operations that require a strictly ordered sequence, like `head` and `tail`, will not be available.  Regardless, the lack of a stable ordering is easily demonstrated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JohnPaulRingoGeorge\n",
      "JohnPaulRingoGeorge\n"
     ]
    }
   ],
   "source": [
    "// stable order for Scala Seq type\n",
    "println(beatles.fold(\"\")(_ + _))\n",
    "println(beatles.fold(\"\")(_ + _))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RingoPaulJohnGeorge\n",
      "JohnPaulGeorgeRingo\n"
     ]
    }
   ],
   "source": [
    "// but not for a parallelized collection\n",
    "println(distributedBeatles.fold(\"\")(_ + _))\n",
    "println(distributedBeatles.fold(\"\")(_ + _))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating `DataFrame`s Programmatically\n",
    "\n",
    "There are a number of ways we can create a `DataFrame`.  Since we just introduced `RDD`s, let us first demonstrate how we can create a `DataFrame` from an `RDD`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m\"George\"\u001b[39m, \u001b[32m\"Harrison\"\u001b[39m),\n",
       "  (\u001b[32m\"Ringo\"\u001b[39m, \u001b[32m\"Starr\"\u001b[39m),\n",
       "  (\u001b[32m\"John\"\u001b[39m, \u001b[32m\"Lennon\"\u001b[39m),\n",
       "  (\u001b[32m\"Paul\"\u001b[39m, \u001b[32m\"McArtney\"\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq(\n",
    "  (\"George\", \"Harrison\"),\n",
    "  (\"Ringo\", \"Starr\"),\n",
    "  (\"John\", \"Lennon\"),\n",
    "  (\"Paul\", \"McArtney\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstName|lastName|\n",
      "+---------+--------+\n",
      "|   George|Harrison|\n",
      "|    Ringo|   Starr|\n",
      "|     John|  Lennon|\n",
      "|     Paul|McArtney|\n",
      "+---------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "sc\n",
    "  .parallelize(data)\n",
    "  .toDF(\"firstName\", \"lastName\")\n",
    "  .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating `DataFrame`s from External Sources\n",
    "\n",
    "Blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "import spark.sql\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "|  nycflights|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|carrier|                name|\n",
      "+-------+--------------------+\n",
      "|carrier|                name|\n",
      "|     9E|   Endeavor Air Inc.|\n",
      "|     AA|American Airlines...|\n",
      "|     AS|Alaska Airlines Inc.|\n",
      "|     B6|     JetBlue Airways|\n",
      "|     DL|Delta Air Lines Inc.|\n",
      "|     EV|ExpressJet Airlin...|\n",
      "|     F9|Frontier Airlines...|\n",
      "|     FL|AirTran Airways C...|\n",
      "|     HA|Hawaiian Airlines...|\n",
      "|     MQ|           Envoy Air|\n",
      "|     OO|SkyWest Airlines ...|\n",
      "|     UA|United Air Lines ...|\n",
      "|     US|     US Airways Inc.|\n",
      "|     VX|      Virgin America|\n",
      "|     WN|Southwest Airline...|\n",
      "|     YV|  Mesa Airlines Inc.|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql(\"SELECT * FROM nycflights.airlines\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{Row, SaveMode, SparkSession}\n",
    "val warehouseLocation = \"/data/hive/warehouse\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.11)",
   "language": "scala",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
