{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    div.container {\n",
    "      max-width: 800px!important;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "# Spark SQL Basics\n",
    "\n",
    "Spark SQL provides the means for working with structured data within Apache Spark.  Structured data is represented by the `DataFrame` abstraction (which is a type alias for `Dataset[Row]`), and we can act on them using familiar-looking SQL queries, or else the `DataFrame` API.  In this lesson, we cover `DataFrame` basics, including:\n",
    "\n",
    "* creating `DataFrame`s in code\n",
    "* creating `DataFrame`s from external sources (CSV, parquet, hive, PostgreSQL, etc.)\n",
    "* manipulating and summarising `DataFrame`s using both SQL and the `DataFrame` API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "This workbook makes use of the [Almond Scala kernel for Jupyter](https://almond.sh/).  To use Spark, we have to first add a few libraries to the classpath, which we can do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36minit\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init: Unit = {\n",
    "  import ammonite.ops._\n",
    "  val jars = ls! root/'opt/'spark/'jars |? (_.ext == \"jar\")\n",
    "  jars.foreach(interp.load.cp(_))   \n",
    "}\n",
    "\n",
    "init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is also pretty verbose with respect to logging, so it can be useful to change the logging policy to de-clutter our outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just to get it out of the way up front, we import a number of objects that we need throughout the rest of the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, sometimes a code block will produce a large amount of output, some of which is unimportant, and so obfuscatory.  To hide this, we sometimes wrap things in an object like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject\u001b[39m \u001b[36mfoo\u001b[39m\n",
       "\u001b[36mres3_1\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m3\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object foo {\n",
    "  val x = 1\n",
    "  val y = 2\n",
    "}\n",
    "\n",
    "foo.x + foo.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `foo` serves no functional purpose here other than to hide the interpreter output that results from the assignment of `x` and `y`.  But doing this means we need to use dot notation to refer to object members.  Another option, which is only really useful when results span multiple lines, is to use Scala's lazy evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">x</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Int</span></span> = <span style=\"color: white\"><span class=\"ansi-white-fg\">[lazy]</span></span>\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">y</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Int</span></span> = <span style=\"color: white\"><span class=\"ansi-white-fg\">[lazy]</span></span>\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">res4_2</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Int</span></span> = <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">3</span></span></code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mx\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[37m[lazy]\u001b[39m\n",
       "\u001b[36my\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[37m[lazy]\u001b[39m\n",
       "\u001b[36mres4_2\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m3\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lazy val x = 1\n",
    "lazy val y = 2\n",
    "x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a `SparkSession`\n",
    "\n",
    "As of Spark 2.x, the usual method of interacting with Spark is by creating `SparkSession` to function as a single entrypoing.  In our case, we do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@101ea05"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "  .builder\n",
    "  .config(\"hive.metastore.uris\",\"thrift://localhost:9083\") \n",
    "  .config(\"spark.sql.warehouse.dir\", \"/data/hive/warehouse\")\n",
    "  .master(\"local[*]\")\n",
    "  .appName(\"Spark SQL Basics\")\n",
    "  .enableHiveSupport()\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we explicitly configure our session to use Hive by setting values for `hive.metastore.uris` and `spark.sql.warehouse.dir`.  It is common for this to work automatically by configuration (via the file `hive-site.xml`), but this does not appear to the case in this context.  We also tell Spark to work in pseudo-distributed mode by setting `master` to `local[*]`.  There are various other configurations possible, but that's out of scope here.  See [Configuration - Spark 2.4.3 Documentation](https://spark.apache.org/docs/latest/configuration.html) for details.\n",
    "\n",
    "When working with Spark SQL, it is very common to use the object `spark.sparkContext`.  So for convenience, we also assign this to a variable, commonly `sc`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@7fc937e0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.b. that we decreased the amount of debugging already in above.  We can also do this via the `SparkSession` object by running `sc.setLogLevel(\"ERROR\")`, but then we'd still be subjected to the logging that occurs as a result of creating the `SparkSession` itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Comment on File Systems\n",
    "\n",
    "Spark can read happily from a local filesystem, and it can also read from the Hadoop distributed file system (HDFS).  If we refer to a file such as `sc.textFile(\"/a/file\")`, it could be a file on either&ndash;which will be determined by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres7\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"file:///\"\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.hadoopConfiguration.get(\"fs.defaultFS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of the value of this parameter, we can explicitly refer to local files with the syntax `file:///path/to/file`, and to files on HDFS as `hdfs://server:port/path/to/file`.  In our case, HDFS is accessible at `hdfs://localhost:9000`, which is configured in `/opt/hadoop/conf/core-site.xml` via the `fs.defaultFS` parameter.  `core-site.xml` will often be read by Spark, though it doesn't seem to be here.  It is probably worth being explicit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Dataset (`RDD`)\n",
    "\n",
    "As noted, `DataFrame` is the central data abstraction when working with structured data.  However, these build on an earlier abstraction called Resilient Distributed Datasets (`RDD`), and one will still have occasion to use these.  An `RDD` is essentially just a normal Scala collection that's been parallelised for use with Spark.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mbeatles\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m\"John\"\u001b[39m, \u001b[32m\"Paul\"\u001b[39m, \u001b[32m\"Ringo\"\u001b[39m, \u001b[32m\"George\"\u001b[39m)\n",
       "\u001b[36mdistributedBeatles\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[0] at parallelize at cmd8.sc:2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val beatles = Seq(\"John\", \"Paul\", \"Ringo\", \"George\")\n",
    "val distributedBeatles = spark.sparkContext.parallelize(beatles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can treat the resulting `RDD` in much the same way as the original collection, but the `RDD` will be worked on in parallel.  This means that when iterating over an `RDD` the order we will process entries will be unstable.  And because of this, certain operations that require a strictly ordered sequence, like `head` and `tail`, will not be available.  Regardless, the lack of a stable ordering is easily demonstrated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JohnPaulRingoGeorge\n",
      "JohnPaulRingoGeorge\n"
     ]
    }
   ],
   "source": [
    "// stable order for Scala Seq type\n",
    "println(beatles.fold(\"\")(_ + _))\n",
    "println(beatles.fold(\"\")(_ + _))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RingoJohnGeorgePaul\n",
      "RingoPaulGeorgeJohn\n"
     ]
    }
   ],
   "source": [
    "// but not for a parallelized collection\n",
    "println(distributedBeatles.fold(\"\")(_ + _))\n",
    "println(distributedBeatles.fold(\"\")(_ + _))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating `DataFrame`s Programmatically\n",
    "\n",
    "There are a number of ways we can create a `DataFrame`.  Since we just introduced `RDD`s, let us first demonstrate how we can create a `DataFrame` from an `RDD`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m\"George\"\u001b[39m, \u001b[32m\"Harrison\"\u001b[39m),\n",
       "  (\u001b[32m\"Ringo\"\u001b[39m, \u001b[32m\"Starr\"\u001b[39m),\n",
       "  (\u001b[32m\"John\"\u001b[39m, \u001b[32m\"Lennon\"\u001b[39m),\n",
       "  (\u001b[32m\"Paul\"\u001b[39m, \u001b[32m\"McArtney\"\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq(\n",
    "  (\"George\", \"Harrison\"),\n",
    "  (\"Ringo\", \"Starr\"),\n",
    "  (\"John\", \"Lennon\"),\n",
    "  (\"Paul\", \"McArtney\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstName|lastName|\n",
      "+---------+--------+\n",
      "|   George|Harrison|\n",
      "|    Ringo|   Starr|\n",
      "|     John|  Lennon|\n",
      "|     Paul|McArtney|\n",
      "+---------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mbeatles\u001b[39m: \u001b[32mDataFrame\u001b[39m = [firstName: string, lastName: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "val beatles = sc\n",
    "  .parallelize(data)\n",
    "  .toDF(\"firstName\", \"lastName\")\n",
    "\n",
    "beatles.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating `DataFrame`s from External Sources\n",
    "\n",
    "`DataFrame` provides a single common interface for working with structured data.  Still, we can create a `DataFrame` from a number of different input types.  Here we cover several common scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV\n",
    "\n",
    "We can read a variety of external formats via the `SparkContext`, and the pattern is largely the same from format to format.  In this case, we read the famous iris dataset which has been saved locally as a csv file as `/data/csv/iris.csv`.  To import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36miris\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Sepal.Length: double, Sepal.Width: double ... 3 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iris = spark\n",
    "  .read\n",
    "  .format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"delimiter\", \",\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"file:///data/csv/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|Sepal.Length|Sepal.Width|Petal.Length|Petal.Width|Species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.limit(5).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, if we are working in a 'Big Data' environment, we might expect that our file is saved in Hadoop, rather than locally on disk.  For example, we could copy our file to the Hadoop filesystem by running the following:\n",
    "\n",
    "```bash\n",
    "$ hadoop hdfs -mkdir /data\n",
    "$ hadoop hdfs -mkdir /data/csv\n",
    "$ hadoop hdfs -put /data/csv/iris.csv /data/csv/\n",
    "```\n",
    "\n",
    "As noted earlier, the Hadoop filesystem in our case is `hdfs://localhost:9000`, and to import from there instead we would just change the import as follows:\n",
    "\n",
    "```scala\n",
    "val iris = spark\n",
    "  .read\n",
    "  .format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"delimiter\", \",\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"hdfs://localhost:9000/user/root/iris.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hive\n",
    "\n",
    "Apache Hive is a data warehouse that, among other things, provides SQL-like access to data stored on Hadoop. The `SparkContext` in this notebook has support for Hive enabled, and so we can query data in Hive tables using standard-looking SQL queries.  We first import `spark.sql` so we can write `sql(<query>)` instead of `spark.sql(<query>)`&ndash;not a huge convencience, but commonly done in other resources and code bases so we include it for consistency.  Then, we list the available databases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/08/06 20:51:33 INFO metastore: Trying to connect to metastore with URI thrift://localhost:9083\n",
      "19/08/06 20:51:33 INFO metastore: Connected to metastore.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "|  nycflights|\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.sql\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.sql\n",
    "sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nycflights` database is pre-populated from [Bureau of Transportation Statistics](https://www.transtats.bts.gov/) data, as provided by the [nycflights13](https://github.com/hadley/nycflights13) R package.  We can list the available tables as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+\n",
      "|  database|tableName|isTemporary|\n",
      "+----------+---------+-----------+\n",
      "|nycflights| airlines|      false|\n",
      "|nycflights| airports|      false|\n",
      "|nycflights|  flights|      false|\n",
      "|nycflights|   planes|      false|\n",
      "|nycflights|  weather|      false|\n",
      "+----------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres16_0\u001b[39m: \u001b[32mDataFrame\u001b[39m = []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(\"use nycflights\")\n",
    "sql(\"show tables\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to pull the whole `airlines` table, for example, and save the results in a `DataFrame` called `airlines`, we just run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|carrier|                name|\n",
      "+-------+--------------------+\n",
      "|     9E|   Endeavor Air Inc.|\n",
      "|     AA|American Airlines...|\n",
      "|     AS|Alaska Airlines Inc.|\n",
      "|     B6|     JetBlue Airways|\n",
      "|     DL|Delta Air Lines Inc.|\n",
      "|     EV|ExpressJet Airlin...|\n",
      "|     F9|Frontier Airlines...|\n",
      "|     FL|AirTran Airways C...|\n",
      "|     HA|Hawaiian Airlines...|\n",
      "|     MQ|           Envoy Air|\n",
      "|     OO|SkyWest Airlines ...|\n",
      "|     UA|United Air Lines ...|\n",
      "|     US|     US Airways Inc.|\n",
      "|     VX|      Virgin America|\n",
      "|     WN|Southwest Airline...|\n",
      "|     YV|  Mesa Airlines Inc.|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mairlines\u001b[39m: \u001b[32mDataFrame\u001b[39m = [carrier: string, name: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val airlines: DataFrame = sql(\"SELECT * FROM nycflights.airlines\")\n",
    "airlines.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also run familiar-looking queries such as joins and grouped summaries.  For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                name|num_flights|\n",
      "+--------------------+-----------+\n",
      "|AirTran Airways C...|       3260|\n",
      "|Alaska Airlines Inc.|        714|\n",
      "|American Airlines...|      32729|\n",
      "|Delta Air Lines Inc.|      48110|\n",
      "|   Endeavor Air Inc.|      18460|\n",
      "|           Envoy Air|      26397|\n",
      "|ExpressJet Airlin...|      54173|\n",
      "|Frontier Airlines...|        685|\n",
      "|Hawaiian Airlines...|        342|\n",
      "|     JetBlue Airways|      54635|\n",
      "|  Mesa Airlines Inc.|        601|\n",
      "|SkyWest Airlines ...|         32|\n",
      "|Southwest Airline...|      12275|\n",
      "|     US Airways Inc.|      20536|\n",
      "|United Air Lines ...|      58665|\n",
      "|      Virgin America|       5162|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mflightsByCarrier\u001b[39m: \u001b[32mDataFrame\u001b[39m = [name: string, num_flights: bigint]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightsByCarrier = sql(\"\"\"\n",
    "select \n",
    "  name, sum(1) as num_flights \n",
    "from \n",
    "  (\n",
    "    select \n",
    "      airlines.name, flights.* \n",
    "    from \n",
    "      nycflights.airlines \n",
    "    inner join \n",
    "      nycflights.flights \n",
    "    on \n",
    "      airlines.carrier = flights.carrier\n",
    "  ) a\n",
    "group by \n",
    "  name \n",
    "order by \n",
    "  name\n",
    "\"\"\")\n",
    "\n",
    "flightsByCarrier.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational Database\n",
    "\n",
    "Because Spark runs on the JVM, we typically access realational databases via JDBC.  In this case, we have a copy of the `nycflights` database in Postgresql, accessible on `localhost:5432` using the username `guest` and password `guest` (in a production environment, the specifics of authentication would likely be different).  So, we first need add the JDBC driver, and there are serveral ways we could do this.  We have a local copy of the driver available as `/usr/share/java/postgresql-jdbc4.jar`, and this can be added for use in this interactive setting as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mammonite.ops._\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ammonite.ops._\n",
    "interp.load.cp(os.Path(\"/usr/share/java/postgresql-jdbc4.jar\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, we could download the dependency from Maven or similar as follows:\n",
    "\n",
    "```scala\n",
    "import $ivy.`org.postgresql::postgresql:42.2.6`\n",
    "```\n",
    "\n",
    "To read the `airlines` table as before, we would then run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject\u001b[39m \u001b[36mpgsetup\u001b[39m"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object pgsetup {\n",
    "  import java.util.Properties\n",
    "\n",
    "  // register Driver implementation with DriverManager\n",
    "  Class.forName(\"org.postgresql.Driver\")\n",
    "\n",
    "  val connectionProperties = new Properties()\n",
    "  connectionProperties.setProperty(\"Driver\", \"org.postgresql.Driver\")\n",
    "  connectionProperties.setProperty(\"user\", \"guest\")\n",
    "  connectionProperties.setProperty(\"password\", \"guest\")\n",
    "    \n",
    "  val url = \"jdbc:postgresql://localhost:5432/nycflights\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|carrier|                name|\n",
      "+-------+--------------------+\n",
      "|     9E|   Endeavor Air Inc.|\n",
      "|     AA|American Airlines...|\n",
      "|     AS|Alaska Airlines Inc.|\n",
      "|     B6|     JetBlue Airways|\n",
      "|     DL|Delta Air Lines Inc.|\n",
      "|     EV|ExpressJet Airlin...|\n",
      "|     F9|Frontier Airlines...|\n",
      "|     FL|AirTran Airways C...|\n",
      "|     HA|Hawaiian Airlines...|\n",
      "|     MQ|           Envoy Air|\n",
      "|     OO|SkyWest Airlines ...|\n",
      "|     UA|United Air Lines ...|\n",
      "|     US|     US Airways Inc.|\n",
      "|     VX|      Virgin America|\n",
      "|     WN|Southwest Airline...|\n",
      "|     YV|  Mesa Airlines Inc.|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mquery\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"(select * from airlines) as airlines\"\u001b[39m\n",
       "\u001b[36mairlines\u001b[39m: \u001b[32mDataFrame\u001b[39m = [carrier: string, name: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query = \"(select * from airlines) as airlines\"\n",
    "\n",
    "val airlines = spark\n",
    "  .read\n",
    "  .jdbc(pgsetup.url, query, pgsetup.connectionProperties)\n",
    "\n",
    "airlines.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet and Optimized Row Columnar (ORC)\n",
    "\n",
    "Parquet and ORC are popular columnar formats&ndash;parquet more so for Spark, and ORC more so for Hive.  Because data is stored in columns, compression algorithms appropriate for specific columns can be applied, and so the formats generally have good to excellent compression performance.  As an example, the airlines database used above is 53.8MB on disk when stored as CSV, but 7.5MB and 6.0MB when stored as ORC and parquet, respectively.  They also tend to perform very well in read applications like grouped aggregates, though don't fare as well in write applications.  Either way, we could repreduce the example above where we calculated the number of flights by airline as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mairlines\u001b[39m: \u001b[32mDataFrame\u001b[39m = [carrier: string, name: string]\n",
       "\u001b[36mflights\u001b[39m: \u001b[32mDataFrame\u001b[39m = [year: int, month: int ... 17 more fields]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val airlines = spark\n",
    "  .read\n",
    "  .format(\"parquet\")\n",
    "  .load(\"file:///data/parquet/nycflights/airlines/\")\n",
    "\n",
    "val flights = spark\n",
    "  .read\n",
    "  .format(\"orc\")\n",
    "  .load(\"file:///data/orc/nycflights/flights/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                name|num_flights|\n",
      "+--------------------+-----------+\n",
      "|AirTran Airways C...|       3260|\n",
      "|Alaska Airlines Inc.|        714|\n",
      "|American Airlines...|      32729|\n",
      "|Delta Air Lines Inc.|      48110|\n",
      "|   Endeavor Air Inc.|      18460|\n",
      "|           Envoy Air|      26397|\n",
      "|ExpressJet Airlin...|      54173|\n",
      "|Frontier Airlines...|        685|\n",
      "|Hawaiian Airlines...|        342|\n",
      "|     JetBlue Airways|      54635|\n",
      "|  Mesa Airlines Inc.|        601|\n",
      "|SkyWest Airlines ...|         32|\n",
      "|Southwest Airline...|      12275|\n",
      "|     US Airways Inc.|      20536|\n",
      "|United Air Lines ...|      58665|\n",
      "|      Virgin America|       5162|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "airlines.as(\"airlines\")\n",
    "  .join(flights.as(\"flights\"), col(\"airlines.carrier\") === col(\"flights.carrier\"), \"inner\")\n",
    "  .groupBy(\"name\")\n",
    "  .count\n",
    "  .withColumnRenamed(\"count\", \"num_flights\")\n",
    "  .orderBy(\"name\")\n",
    "  .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving `DataFrame`s\n",
    "\n",
    "Saving a `DataFrame` is relatively straightforward.  For example, to save the `beatles` `DataFrame` in CSV format we'd run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SaveMode\n",
       "\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "\n",
    "beatles\n",
    "  .write\n",
    "  .mode(SaveMode.Overwrite)\n",
    "  .format(\"csv\")\n",
    "  .save(\"file:///notebooks/scratch/beatles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, though, things might not be as one expects.  In this case, for example, what we get is a folder called `beatles` with the following content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/scratch/beatles/part-00000-cecaa9d0-e840-4ea1-8907-ceb76ad3ce29-c000.csv\n",
      "/notebooks/scratch/beatles/part-00001-cecaa9d0-e840-4ea1-8907-ceb76ad3ce29-c000.csv\n",
      "/notebooks/scratch/beatles/part-00007-cecaa9d0-e840-4ea1-8907-ceb76ad3ce29-c000.csv\n",
      "/notebooks/scratch/beatles/part-00003-cecaa9d0-e840-4ea1-8907-ceb76ad3ce29-c000.csv\n",
      "/notebooks/scratch/beatles/part-00005-cecaa9d0-e840-4ea1-8907-ceb76ad3ce29-c000.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36md\u001b[39m: \u001b[32mjava\u001b[39m.\u001b[32mio\u001b[39m.\u001b[32mFile\u001b[39m = /notebooks/scratch/beatles"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d = new java.io.File(\"/notebooks/scratch/beatles\")\n",
    "d.listFiles.filter(_.toString endsWith \".csv\").foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a result of working with a local filesystem in distributed mode.  That is, the `DataFrame` is a parallelized collection, and we can't very well have different workers attempting to write to the same file.  If we instead work with a distributed files system such as HDFS, things will be as we expect: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "beatles\n",
    "  .write\n",
    "  .mode(SaveMode.Overwrite)\n",
    "  .format(\"csv\")\n",
    "  .save(\"hdfs://localhost:9000/user/root/beatles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interacting with HDFS is a little convoluted, but we can confirm that this appears as a single file in Hadoop as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://localhost:9000/user/root/beatles.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject\u001b[39m \u001b[36mls\u001b[39m"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object ls {\n",
    "  import org.apache.hadoop.conf.Configuration\n",
    "  import org.apache.hadoop.fs._\n",
    "\n",
    "  val config = new Configuration()\n",
    "  config.set(\"fs.defaultFS\", \"hdfs://localhost:9000\")\n",
    "  val hdfs = org.apache.hadoop.fs.FileSystem.get(config)\n",
    "  val path = new org.apache.hadoop.fs.Path(\"/user/root\")\n",
    "}\n",
    "\n",
    "ls.hdfs.listStatus(ls.path).foreach(f => println(f.getPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions and Transformations\n",
    "\n",
    "In broad terms, any method you call on a `DataFrame` will be an _action_ or a _transformation_.  Transformations are essentially instructions that tell Spark how to take one or more `DataFrame`s (or `RDD`s) and _transform_ them into something else.  Crucially, Spark does not evaluate such transformation right away, but rather saves them up until evaluation is necessary.  This is an example of _lazy evaluation_, and doing this means that Spark can find the most efficient evaluation plan.  For example, say we wanted to calculate the number of flights by airline, and then select the results for one specific airline.  Clearly, if we saved both tasks up we'd see that it would be more efficient to filter for the one specific airline first, and then count the flights; rather than execute each task in turn.  Actions, on the other hand, result in evaluation.\n",
    "\n",
    "Transformations generally yield `DataFrame`s, and they are very useful.  For example, we can break a large query up into several smaller queries, and each of the smaller queries can be reused in other larger queries.  We'll see example of this below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with `DataFrame`s / The SQL API\n",
    "\n",
    "In this last section, we cover a number of common use cases&ndash;filtering, joining, and so on.  Most things have an obvious SQL analogue, though this isn't always the case.  User defined functions exist for a number of different relational database, for example, but none so elegantly as for `DataFrame`s.  In addition, implementations of familiar Scala methods such as `map` and `flatMap` are available.  \n",
    "\n",
    "Throughout this section, we again make use of the `nycflights` data.  One nice way of doing this, which has the added advantage when working in an interactive setting of producing much less output, is to use an object.  That way, we get, for example, `DataFrame`s we access by via `nycflights.airlines`, rather than `airlines`, which feels a bit database-like.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject\u001b[39m \u001b[36mnycflights\u001b[39m"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object nycflights {\n",
    "  private def read(fname: String): DataFrame = {\n",
    "    spark \n",
    "      .read\n",
    "      .format(\"parquet\")\n",
    "      .load(s\"file:///data/parquet/nycflights/${fname}/\")\n",
    "  }\n",
    "\n",
    "  val airlines = read(\"airlines\")\n",
    "  val airports = read(\"airports\")\n",
    "  val flights = read(\"flights\")\n",
    "  val planes = read(\"planes\")\n",
    "  val weather = read(\"weather\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we'd rather not use dot notation, we can just reassign the `DataFrame`s.  The assignment is by-reference, so still efficient; and the objects are immutable, so it's all safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mairlines\u001b[39m: \u001b[32mDataFrame\u001b[39m = [carrier: string, name: string]\n",
       "\u001b[36mairports\u001b[39m: \u001b[32mDataFrame\u001b[39m = [faa: string, name: string ... 6 more fields]\n",
       "\u001b[36mflights\u001b[39m: \u001b[32mDataFrame\u001b[39m = [year: int, month: int ... 17 more fields]\n",
       "\u001b[36mplanes\u001b[39m: \u001b[32mDataFrame\u001b[39m = [tailnum: string, year: int ... 7 more fields]\n",
       "\u001b[36mweather\u001b[39m: \u001b[32mDataFrame\u001b[39m = [origin: string, year: int ... 13 more fields]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val airlines = nycflights.airlines\n",
    "val airports = nycflights.airports\n",
    "val flights = nycflights.flights\n",
    "val planes = nycflights.planes\n",
    "val weather = nycflights.weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing a `DataFrame`\n",
    "\n",
    "Usefully, we can examine the schema of a `DataFrame` via the `schema` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructField(faa,StringType,true)\n",
      "StructField(name,StringType,true)\n",
      "StructField(lat,FloatType,true)\n",
      "StructField(lon,FloatType,true)\n",
      "StructField(alt,IntegerType,true)\n",
      "StructField(tz,IntegerType,true)\n",
      "StructField(dst,StringType,true)\n",
      "StructField(tzone,StringType,true)\n"
     ]
    }
   ],
   "source": [
    "nycflights.airports.schema.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that the types, `*Type`, here refer to actual classes defined in `org.apache.spark.sql.types`&ndash;it would be a useful exercise to look through these as they are essentially all the types you can legally use as columns in a `DataFrame`).  If we just want column names, we can get those as an array by calling the `columns` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres31\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"faa\"\u001b[39m,\n",
       "  \u001b[32m\"name\"\u001b[39m,\n",
       "  \u001b[32m\"lat\"\u001b[39m,\n",
       "  \u001b[32m\"lon\"\u001b[39m,\n",
       "  \u001b[32m\"alt\"\u001b[39m,\n",
       "  \u001b[32m\"tz\"\u001b[39m,\n",
       "  \u001b[32m\"dst\"\u001b[39m,\n",
       "  \u001b[32m\"tzone\"\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nycflights.airports.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is valuable in a programmatic setting, though for exploratory purposes it is sometimes more useful to simply view a subset of the table.  We've already seen this in action above, but to show just the first 10 rows of the `airports` table, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------+---------+----+---+---+----------------+\n",
      "|faa|                name|      lat|      lon| alt| tz|dst|           tzone|\n",
      "+---+--------------------+---------+---------+----+---+---+----------------+\n",
      "|04G|   Lansdowne Airport|41.130474|-80.61958|1044| -5|  A|America/New_York|\n",
      "|06A|Moton Field Munic...| 32.46057|-85.68003| 264| -6|  A| America/Chicago|\n",
      "|06C| Schaumburg Regional| 41.98934|-88.10124| 801| -6|  A| America/Chicago|\n",
      "|06N|     Randall Airport| 41.43191|-74.39156| 523| -5|  A|America/New_York|\n",
      "|09J|Jekyll Island Air...|31.074472|-81.42778|  11| -5|  A|America/New_York|\n",
      "+---+--------------------+---------+---------+----+---+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nycflights.airports\n",
    "  .limit(5)\n",
    "  .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `nycflights.airports.limit(10)` is a transformation, and simply yields another `DataFrame`.  But `show` forces Spark to retrieve the requested rows and print them, so is an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with columns\n",
    "\n",
    "As observed above, the columns in the `airports` `DataFrame` are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres33\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"faa, name, lat, lon, alt, tz, dst, tzone\"\u001b[39m"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports.columns.mkString(\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we want to rename `lat` to `latitude`, `lon` to `longitude`, and that we only want to retain `name`, `latitude`, and `longitude`.  We can use the `withColumnRenamed` method to rename columns, and `select` to list the columns we want to keep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------+\n",
      "|                name| latitude|longitude|\n",
      "+--------------------+---------+---------+\n",
      "|   Lansdowne Airport|41.130474|-80.61958|\n",
      "|Moton Field Munic...| 32.46057|-85.68003|\n",
      "| Schaumburg Regional| 41.98934|-88.10124|\n",
      "|     Randall Airport| 41.43191|-74.39156|\n",
      "|Jekyll Island Air...|31.074472|-81.42778|\n",
      "+--------------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports\n",
    "  .withColumnRenamed(\"lat\", \"latitude\")\n",
    "  .withColumnRenamed(\"lon\", \"longitude\")\n",
    "  .select(\"name\", \"latitude\", \"longitude\")\n",
    "  .limit(5)\n",
    "  .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, `withColumnRenamed` and `select` both take a comma separated lists of `String`s as arguments.  But `String` is an existing type, with limited built-in methods available.  To allow us to do interesting things with columns, we need to refer to them as type [`Column`](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.Column).  We can do this several ways.  For example, to refer to the `name` column in the `airports` table we'd use one of:\n",
    "\n",
    "* `col(\"name\")`\n",
    "* `column(\"name\")`\n",
    "* `$\"name\"` (actually has type [`ColumnName`](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.ColumnName), which is a subtype of `Column`)\n",
    "* `airports(\"name\")`, which is short-hand for `airports.apply(\"name\")`\n",
    "\n",
    "where `col` and `column` are defined in [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$).  The `Column` type has a number of functions associated with it, and can be used to compose complex expressions.  A subset are as follows:\n",
    "\n",
    "operator | meaning                    | examples\n",
    "---------|----------------------------|---------------\n",
    "`lit`    | literal                    | `lit(1)`, `lit(\"foo\")`, `lit(java.sql.Date.valueOf(\"2019-01-01\"))`\n",
    "`%`      | modulo                     | `col(\"a\") % 2`\n",
    "`&&`     | logical and                | `lit(true) && lit(false)`\n",
    "`\\|\\|`   | logical or                 | `lit(true) \\|\\| lit(false)` \n",
    "`===`    | equality                   | `lit(1) === lit(1)`, `col(\"a\") === col(\"b\")`\n",
    "`<=>`    | equality (ignores `null`)  | `lit(1) <=> lit(1)`, `col(\"a\") <=> col(\"b\")`\n",
    "`=!=`    | inequality                 | `col(\"a\") =!= col(\"b\")`\n",
    "`<`      | less than                  | `lit(1) < lit(2)`, `col(\"x\") < col(\"b\")`\n",
    "`<=`     | less than or equal         | `lit(1) <= lit(2)`, `col(\"x\") <= col(\"b\")`\n",
    "`>`      | greater than               | `lit(1) > lit(2)`, `col(\"a\") > col(\"b\")`\n",
    "`>=`     | greater than or equal      | `lit(1) >= lit(2)`, `col(\"a\") >= col(\"b\")`\n",
    "`*`      | mulitplication             | `lit(1) * lit(2)`, `col(\"a\") * col(\"b\")`\n",
    "`/`      | division                   | `lit(1) / lit(2)`, `col(\"a\") / col(\"b\")`\n",
    "`+`      | addition                   | `lit(1) + lit(2)`, `col(\"a\") + col(\"b\")`\n",
    "`-`      | subtraction                | `lit(1) - lit(2)`, `col(\"a\") - col(\"b\")`\n",
    "`as`     | assign alias / rename      | `col(\"foo\") as \"bar\"`\n",
    "`alias`  | assign alias / rename      | `col(\"foo\") alias \"bar\"`\n",
    "\n",
    "So, we could rename the `lat` and `lon` columns as above in the alternative fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------+\n",
      "|                name| latitude|longitude|\n",
      "+--------------------+---------+---------+\n",
      "|   Lansdowne Airport|41.130474|-80.61958|\n",
      "|Moton Field Munic...| 32.46057|-85.68003|\n",
      "| Schaumburg Regional| 41.98934|-88.10124|\n",
      "|     Randall Airport| 41.43191|-74.39156|\n",
      "|Jekyll Island Air...|31.074472|-81.42778|\n",
      "+--------------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports\n",
    "  .select(col(\"name\"), col(\"lat\").as(\"latitude\"), col(\"lon\") as (\"longitude\"))\n",
    "  .limit(5)\n",
    "  .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can use `col` and the like in the same way as any other method&ndash;applying them to a collection, for example.  The following converts each column name to upper case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------+---------+----+---+---+----------------+\n",
      "|FAA|                NAME|      LAT|      LON| ALT| TZ|DST|           TZONE|\n",
      "+---+--------------------+---------+---------+----+---+---+----------------+\n",
      "|04G|   Lansdowne Airport|41.130474|-80.61958|1044| -5|  A|America/New_York|\n",
      "|06A|Moton Field Munic...| 32.46057|-85.68003| 264| -6|  A| America/Chicago|\n",
      "|06C| Schaumburg Regional| 41.98934|-88.10124| 801| -6|  A| America/Chicago|\n",
      "|06N|     Randall Airport| 41.43191|-74.39156| 523| -5|  A|America/New_York|\n",
      "|09J|Jekyll Island Air...|31.074472|-81.42778|  11| -5|  A|America/New_York|\n",
      "+---+--------------------+---------+---------+----+---+---+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">cols</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Array</span></span>[<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">String</span></span>] = <span style=\"color: white\"><span class=\"ansi-white-fg\">[lazy]</span></span>\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">colsUpperCased</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Array</span></span>[<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Column</span></span>] = <span style=\"color: white\"><span class=\"ansi-white-fg\">[lazy]</span></span></code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mcols\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[37m[lazy]\u001b[39m\n",
       "\u001b[36mcolsUpperCased\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mColumn\u001b[39m] = \u001b[37m[lazy]\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lazy val cols = airports.columns\n",
    "lazy val colsUpperCased = cols.map(c => col(c) as c.toUpperCase)\n",
    "airports.select(colsUpperCased: _*).limit(5).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example less straight forward if the user is not that proficient with Scala, though the intent is hopefully obvious enough.  The first line simply gets an `Array` containing the column names as `String`s.  The second line applies a function to each element of the array, and returns an array containing the result of the function.  The function itself is just a mapping from variable `c` to `col(c).as(c.toUpperCase)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres37\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mColumn\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  faa AS `FAA`,\n",
       "  name AS `NAME`,\n",
       "  lat AS `LAT`,\n",
       "  lon AS `LON`,\n",
       "  alt AS `ALT`,\n",
       "  tz AS `TZ`,\n",
       "  dst AS `DST`,\n",
       "  tzone AS `TZONE`\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols.map(c => col(c) as c.toUpperCase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final line, we pass in the array containing the renaming expressions, and the `_*` decoration tells the compiler to treat the array as though it was a comma separated list.  That is:\n",
    "\n",
    "```scala\n",
    "foo(Array(1,2,3): _*)\n",
    "```\n",
    "\n",
    "is equivalent to:\n",
    "\n",
    "```scala\n",
    "foo(1,2,3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregates\n",
    "\n",
    "Aggregates are most easily derived using the `agg` method.  The method takes a comma separated list of aggregate columns (aggregate functions applied to columns or column names), and there is an impressive set of aggregate functions that are provided.  As an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+-----+-----+--------+\n",
      "|    n|  min|   max| mean|   sd|kurtosis|\n",
      "+-----+-----+------+-----+-----+--------+\n",
      "|26115|10.94|100.04|55.26|17.79|   -0.98|\n",
      "+-----+-----+------+-----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather\n",
    "  .agg(\n",
    "    sum(lit(1))                as \"n\", \n",
    "    min(\"temp\")                as \"min\", \n",
    "    max(\"temp\")                as \"max\", \n",
    "    round(mean(\"temp\"), 2)     as \"mean\", \n",
    "    round(stddev(\"temp\"), 2)   as \"sd\", \n",
    "    round(kurtosis(\"temp\"), 2) as \"kurtosis\"\n",
    "  )\n",
    "  .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same thing can be achieved for groups simply by adding the `groupBy` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+-----+------+-----+-----+--------+\n",
      "|origin|month|  n|  min|   max| mean|   sd|kurtosis|\n",
      "+------+-----+---+-----+------+-----+-----+--------+\n",
      "|   JFK|    8|738|60.08| 87.08|73.82| 4.76| -0.0795|\n",
      "|   EWR|    8|740| 59.0| 89.96|74.54| 5.87| -0.2081|\n",
      "|   LGA|    1|742|12.02|  59.0|35.96| 9.88|  -0.467|\n",
      "|   EWR|   12|714|17.96|  71.6|37.95|11.12| -0.0266|\n",
      "|   EWR|    4|720|30.92| 84.02|52.98|  9.6|  0.1855|\n",
      "|   JFK|   11|713| 23.0| 66.92|45.13|10.08| -0.7933|\n",
      "|   EWR|    7|741|64.04|100.04| 80.7| 7.37| -0.3601|\n",
      "|   LGA|    5|744|44.96| 93.02|62.75| 9.86|  0.1686|\n",
      "|   EWR|    9|719|48.02|  95.0| 67.3| 9.32| -0.4419|\n",
      "|   JFK|    6|720|53.96|  89.6|69.96| 6.45| -0.2422|\n",
      "|   LGA|    7|743|64.94| 98.96|80.76| 7.23| -0.4944|\n",
      "|   LGA|    8|739|62.06| 89.06|75.05|  4.8|  0.0421|\n",
      "|   JFK|    3|742|26.96| 57.92|39.54| 6.02| -0.4816|\n",
      "|   EWR|    3|743|26.06| 60.08|40.12| 6.72| -0.2136|\n",
      "|   EWR|    6|720|55.04| 93.92|73.27| 8.05| -0.5447|\n",
      "|   LGA|   10|738|42.08| 84.92|60.63| 8.15|   0.008|\n",
      "|   JFK|    7|744|64.04| 98.06|78.73| 6.55|  0.0473|\n",
      "|   JFK|    2|671|17.06|  50.0|34.19| 6.98|  -0.603|\n",
      "|   LGA|    2|670|19.04| 51.98|34.36| 6.69| -0.5447|\n",
      "|   LGA|    3|742|28.94| 57.02|39.98| 5.97| -0.4251|\n",
      "+------+-----+---+-----+------+-----+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather\n",
    "  .groupBy(\"origin\", \"month\")\n",
    "  .agg(\n",
    "    sum(lit(1))                as \"n\", \n",
    "    min(\"temp\")                as \"min\", \n",
    "    max(\"temp\")                as \"max\", \n",
    "    round(mean(\"temp\"), 2)     as \"mean\", \n",
    "    round(stddev(\"temp\"), 2)   as \"sd\", \n",
    "    round(kurtosis(\"temp\"), 4) as \"kurtosis\"\n",
    "  )\n",
    "  .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining tables\n",
    "\n",
    "Joins are probably better understood using smaller, illustrative datasets.  In this case, let us just programmatically create `DataFrame`s called `A` and `B` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject\u001b[39m \u001b[36mjoins\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjoins._\n",
       "\u001b[39m\n",
       "\u001b[36mA\u001b[39m: \u001b[32mDataFrame\u001b[39m = [x: string, u: int ... 1 more field]\n",
       "\u001b[36mB\u001b[39m: \u001b[32mDataFrame\u001b[39m = [x: string, v: int ... 1 more field]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object joins {\n",
    "  import scala.util.Random\n",
    "\n",
    "  val A = sc.parallelize(\n",
    "    Seq(\"a\", \"b\", \"c\")\n",
    "      .map(x => (x, Random.nextInt(10), Random.nextInt(10))) \n",
    "  ).toDF(\"x\", \"u\", \"v\")\n",
    "\n",
    "  val B = sc.parallelize(\n",
    "    Seq(\"b\", \"c\", \"d\")\n",
    "      .map(x => (x, Random.nextInt(10), Random.nextInt(10))) \n",
    "  ).toDF(\"x\", \"v\", \"w\")\n",
    "}\n",
    "\n",
    "import joins._\n",
    "A.show\n",
    "B.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An inner join then looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+\n",
      "|  x|  u|  v|  v|  w|\n",
      "+---+---+---+---+---+\n",
      "|  c|  3|  6|  2|  3|\n",
      "|  b|  6|  9|  0|  5|\n",
      "+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A\n",
    "  .join(B, Seq(\"x\"), \"inner\")\n",
    "  .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that `v` is duplicated, which is the correct behaviour if we are not also using `v` in the join.  This will lead to problems with ambiguous column names, so it is usually best to alias duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+\n",
      "|  x|  u|v_A|v_B|  w|\n",
      "+---+---+---+---+---+\n",
      "|  c|  3|  6|  2|  3|\n",
      "|  b|  6|  9|  0|  5|\n",
      "+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A\n",
    "  .withColumnRenamed(\"v\", \"v_A\")\n",
    "  .join(B.withColumnRenamed(\"v\", \"v_B\"), Seq(\"x\"), \"inner\")\n",
    "  .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it useful to do something like an implicit join in SQL.  In this case the joining variable is common, and so it is duplicated also, and so this approach is possibly more useful when you want to join two tables where the names of the joining columns do not match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+\n",
      "|  x|  u|  v|  x|  v|  w|\n",
      "+---+---+---+---+---+---+\n",
      "|  c|  3|  6|  c|  2|  3|\n",
      "|  b|  6|  9|  b|  0|  5|\n",
      "+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A\n",
    "  .join(B, A(\"x\") === B(\"x\"))\n",
    "  .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+\n",
      "|  u|  v|  x|  v|  w|\n",
      "+---+---+---+---+---+\n",
      "|  3|  6|  c|  2|  3|\n",
      "|  6|  9|  b|  0|  5|\n",
      "+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A\n",
    "  .withColumnRenamed(\"x\", \"x_A\")\n",
    "  .join(B, col(\"x_A\") === col(\"x\"))\n",
    "  .drop(\"x_A\")\n",
    "  .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User defined functions (UDFs)\n",
    "\n",
    "Having the ability to easily create your own functions that can run over the columns in a `DataTable` is an absolutely killer feature&ndash;perhaps not exciting for those accustomed to using data frame abstractions in languages such as R or Python, but probably for those who've had to do similar things with relational databases.  Consider the weather table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+\n",
      "|origin|month|temp_F|\n",
      "+------+-----+------+\n",
      "|   EWR|    1| 39.02|\n",
      "|   EWR|    1| 39.02|\n",
      "|   EWR|    1| 39.02|\n",
      "|   EWR|    1| 39.92|\n",
      "|   EWR|    1| 39.02|\n",
      "+------+-----+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mweathersub\u001b[39m: \u001b[32mDataFrame\u001b[39m = [origin: string, month: int ... 1 more field]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weathersub = weather\n",
    "  .select(col(\"origin\"), col(\"month\"), col(\"temp\") as \"temp_F\")\n",
    "  \n",
    "weathersub\n",
    "  .limit(5)\n",
    "  .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `temp` appears to be air temperature in degrees Farenheit.  Assume we want to add a column called `temp_C` which contained air temperature in degrees celsius.  We could do this using functions we've already seen.  Given the formula $(x  32) \\times 5/9$, we could write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+------+\n",
      "|origin|month|temp_F|temp_C|\n",
      "+------+-----+------+------+\n",
      "|   EWR|    1| 39.02|   3.9|\n",
      "|   EWR|    1| 39.02|   3.9|\n",
      "|   EWR|    1| 39.02|   3.9|\n",
      "|   EWR|    1| 39.92|   4.4|\n",
      "|   EWR|    1| 39.02|   3.9|\n",
      "+------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weathersub\n",
    "  .withColumn(\"temp_C\", round((col(\"temp_F\") - 32) * lit(5.0/9.0), 2))\n",
    "  .limit(5)\n",
    "  .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+------+\n",
      "|origin|month|temp_F|temp_C|\n",
      "+------+-----+------+------+\n",
      "|   EWR|    1| 39.02|   3.9|\n",
      "|   EWR|    1| 39.02|   3.9|\n",
      "|   EWR|    1| 39.02|   3.9|\n",
      "|   EWR|    1| 39.92|   4.4|\n",
      "|   EWR|    1| 39.02|   3.9|\n",
      "+------+-----+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">toCelsius</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Double</span></span> =&gt; <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Double</span></span> = <span style=\"color: white\"><span class=\"ansi-white-fg\">[lazy]</span></span>\n",
       "<span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">toCelsiusUDF</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">expressions</span></span>.<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">UserDefinedFunction</span></span> = <span style=\"color: white\"><span class=\"ansi-white-fg\">[lazy]</span></span></code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mtoCelsius\u001b[39m: \u001b[32mDouble\u001b[39m => \u001b[32mDouble\u001b[39m = \u001b[37m[lazy]\u001b[39m\n",
       "\u001b[36mtoCelsiusUDF\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mUserDefinedFunction\u001b[39m = \u001b[37m[lazy]\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lazy val toCelsius = (x: Double) => (x - 32) * 5 / 9\n",
    "lazy val toCelsiusUDF = udf(toCelsius)\n",
    "\n",
    "weathersub\n",
    "  .withColumn(\"temp_C\", round(toCelsiusUDF(col(\"temp_F\")), 2))\n",
    "  .limit(5)\n",
    "  .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User defined aggregate functions (UDAFs)\n",
    "\n",
    "e.g. [UDAF](https://docs.databricks.com/spark/latest/spark-sql/udaf-scala.html)\n",
    "\n",
    "<!--build a median function? -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtimeit\u001b[39m"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def timeit[T](block: =>T): (T, Double) = {\n",
    "  val startTime = System.currentTimeMillis()\n",
    "  val res: T = block\n",
    "  (res, System.currentTimeMillis() - startTime)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.11)",
   "language": "scala",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
