{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    div.container {\n",
    "      max-width: 800px!important;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "# Spark SQL Basics\n",
    "\n",
    "Spark SQL provides the means for working with structured data within Apache Spark.  Structured data is represented by the `DataFrame` abstraction (which is a type alias for `Dataset[Row]`), and we can act on them using familiar-looking SQL queries, or else the `DataFrame` API.  In this lesson, we cover `DataFrame` basics, including:\n",
    "\n",
    "* creating `DataFrame`s in code\n",
    "* creating `DataFrame`s from external sources (CSV, parquet, hive, PostgreSQL, etc.)\n",
    "* manipulating and summarising `DataFrame`s using both SQL and the `DataFrame` API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "This workbook makes use of the [Almond Scala kernel for Jupyter](https://almond.sh/).  To use Spark, we have to first add a few libraries to the classpath, which we can do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36minit\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init: Unit = {\n",
    "  import ammonite.ops._\n",
    "  val jars = ls! root/'opt/'spark/'jars |? (_.ext == \"jar\")\n",
    "  jars.foreach(interp.load.cp(_))   \n",
    "}\n",
    "\n",
    "init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is also pretty verbose with respect to logging, so it can be useful to change the logging policy to de-clutter our outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just to get it out of the way up front, we import a number of objects that we need throughout the rest of the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, sometimes a code block will produce a large amount of output, some of which is unimportant, and so obfuscatory.  To hide this, we sometimes wrap things in an object like so:\n",
    "\n",
    "```scala\n",
    "object foo {\n",
    "  val x = 1\n",
    "  val y = 2\n",
    "}\n",
    "\n",
    "x + y\n",
    "```\n",
    "\n",
    "The object `foo` serves no functional purpose here other than to hide the interpreter output that results from the assignment of `x` and `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a `SparkSession`\n",
    "\n",
    "As of Spark 2.x, the usual method of interacting with Spark is by creating `SparkSession` to function as a single entrypoing.  In our case, we do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@7a0ce2df"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "  .builder\n",
    "  .config(\"hive.metastore.uris\",\"thrift://localhost:9083\") \n",
    "  .config(\"spark.sql.warehouse.dir\", \"/data/hive/warehouse\")\n",
    "  .master(\"local[*]\")\n",
    "  .appName(\"Spark SQL Basics\")\n",
    "  .enableHiveSupport()\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we explicitly configure our session to use Hive by setting values for `hive.metastore.uris` and `spark.sql.warehouse.dir`.  It is common for this to work automatically by configuration (via the file `hive-site.xml`), but this does not appear to the case in this context.  We also tell Spark to work in pseudo-distributed mode by setting `master` to `local[*]`.  There are various other configurations possible, but that's out of scope here.  See [Configuration - Spark 2.4.3 Documentation](https://spark.apache.org/docs/latest/configuration.html) for details.\n",
    "\n",
    "When working with Spark SQL, it is very common to use the object `spark.sparkContext`.  So for convenience, we also assign this to a variable, commonly `sc`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@43bc9178"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.b. that we decreased the amount of debugging already in above.  We can also do this via the `SparkSession` object by running `sc.setLogLevel(\"ERROR\")`, but then we'd still be subjected to the logging that occurs as a result of creating the `SparkSession` itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Dataset (`RDD`)\n",
    "\n",
    "As noted, `DataFrame` is the central data abstraction when working with structured data.  However, these build on an earlier abstraction called Resilient Distributed Datasets (`RDD`), and one will still have occasion to use these.  An `RDD` is essentially just a normal Scala collection that's been parallelised for use with Spark.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mbeatles\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m\"John\"\u001b[39m, \u001b[32m\"Paul\"\u001b[39m, \u001b[32m\"Ringo\"\u001b[39m, \u001b[32m\"George\"\u001b[39m)\n",
       "\u001b[36mdistributedBeatles\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = ParallelCollectionRDD[0] at parallelize at cmd6.sc:2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val beatles = Seq(\"John\", \"Paul\", \"Ringo\", \"George\")\n",
    "val distributedBeatles = spark.sparkContext.parallelize(beatles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can treat the resulting `RDD` in much the same way as the original collection, but the `RDD` will be worked on in parallel.  This means that when iterating over an `RDD` the order we will process entries will be unstable.  And because of this, certain operations that require a strictly ordered sequence, like `head` and `tail`, will not be available.  Regardless, the lack of a stable ordering is easily demonstrated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JohnPaulRingoGeorge\n",
      "JohnPaulRingoGeorge\n"
     ]
    }
   ],
   "source": [
    "// stable order for Scala Seq type\n",
    "println(beatles.fold(\"\")(_ + _))\n",
    "println(beatles.fold(\"\")(_ + _))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JohnRingoPaulGeorge\n",
      "PaulJohnRingoGeorge\n"
     ]
    }
   ],
   "source": [
    "// but not for a parallelized collection\n",
    "println(distributedBeatles.fold(\"\")(_ + _))\n",
    "println(distributedBeatles.fold(\"\")(_ + _))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating `DataFrame`s Programmatically\n",
    "\n",
    "There are a number of ways we can create a `DataFrame`.  Since we just introduced `RDD`s, let us first demonstrate how we can create a `DataFrame` from an `RDD`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m\"George\"\u001b[39m, \u001b[32m\"Harrison\"\u001b[39m),\n",
       "  (\u001b[32m\"Ringo\"\u001b[39m, \u001b[32m\"Starr\"\u001b[39m),\n",
       "  (\u001b[32m\"John\"\u001b[39m, \u001b[32m\"Lennon\"\u001b[39m),\n",
       "  (\u001b[32m\"Paul\"\u001b[39m, \u001b[32m\"McArtney\"\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq(\n",
    "  (\"George\", \"Harrison\"),\n",
    "  (\"Ringo\", \"Starr\"),\n",
    "  (\"John\", \"Lennon\"),\n",
    "  (\"Paul\", \"McArtney\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstName|lastName|\n",
      "+---------+--------+\n",
      "|   George|Harrison|\n",
      "|    Ringo|   Starr|\n",
      "|     John|  Lennon|\n",
      "|     Paul|McArtney|\n",
      "+---------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "sc\n",
    "  .parallelize(data)\n",
    "  .toDF(\"firstName\", \"lastName\")\n",
    "  .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating `DataFrame`s from External Sources\n",
    "\n",
    "`DataFrame` provides a single common interface for working with structured data.  Still, we can create a `DataFrame` from a number of different input types.  Here we cover several common scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive\n",
    "\n",
    "Apache Hive is a data warehouse that, among other things, provides SQL-like access to data stored on Hadoop. The `SparkContext` in this notebook has support for Hive enabled, and so we can query data in Hive tables using standard-looking SQL queries.  We first import `spark.sql` so we can write `sql(<query>)` instead of `spark.sql(<query>)`&ndash;not a huge convencience, but commonly done in other resources and code bases so we include it for consistency.  Then, we list the available databases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/07/23 22:54:26 INFO metastore: Trying to connect to metastore with URI thrift://localhost:9083\n",
      "19/07/23 22:54:26 INFO metastore: Connected to metastore.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "|  nycflights|\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.sql\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.sql\n",
    "sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nycflights` database is pre-populated from [Bureau of Transportation Statistics](https://www.transtats.bts.gov/) data, as provided by the [nycflights13](https://github.com/hadley/nycflights13) R package.  We can list the available tables as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+\n",
      "|  database|tableName|isTemporary|\n",
      "+----------+---------+-----------+\n",
      "|nycflights| airlines|      false|\n",
      "|nycflights| airports|      false|\n",
      "|nycflights|  flights|      false|\n",
      "|nycflights|   planes|      false|\n",
      "|nycflights|  weather|      false|\n",
      "+----------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres12_0\u001b[39m: \u001b[32mDataFrame\u001b[39m = []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(\"use nycflights\")\n",
    "sql(\"show tables\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to pull the whole `airlines` table, for example, and save the results in a `DataFrame` called `airlines`, we just run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|carrier|                name|\n",
      "+-------+--------------------+\n",
      "|     9E|   Endeavor Air Inc.|\n",
      "|     AA|American Airlines...|\n",
      "|     AS|Alaska Airlines Inc.|\n",
      "|     B6|     JetBlue Airways|\n",
      "|     DL|Delta Air Lines Inc.|\n",
      "|     EV|ExpressJet Airlin...|\n",
      "|     F9|Frontier Airlines...|\n",
      "|     FL|AirTran Airways C...|\n",
      "|     HA|Hawaiian Airlines...|\n",
      "|     MQ|           Envoy Air|\n",
      "|     OO|SkyWest Airlines ...|\n",
      "|     UA|United Air Lines ...|\n",
      "|     US|     US Airways Inc.|\n",
      "|     VX|      Virgin America|\n",
      "|     WN|Southwest Airline...|\n",
      "|     YV|  Mesa Airlines Inc.|\n",
      "+-------+--------------------+\n",
      "\n",
      "StructField(carrier,StringType,true)\n",
      "StructField(name,StringType,true)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mairlines\u001b[39m: \u001b[32mDataFrame\u001b[39m = [carrier: string, name: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val airlines: DataFrame = sql(\"SELECT * FROM nycflights.airlines\")\n",
    "airlines.show\n",
    "airlines.schema.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also run familiar-looking queries such as joins and grouped summaries.  For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                name|num_flights|\n",
      "+--------------------+-----------+\n",
      "|AirTran Airways C...|       3260|\n",
      "|Alaska Airlines Inc.|        714|\n",
      "|American Airlines...|      32729|\n",
      "|Delta Air Lines Inc.|      48110|\n",
      "|   Endeavor Air Inc.|      18460|\n",
      "|           Envoy Air|      26397|\n",
      "|ExpressJet Airlin...|      54173|\n",
      "|Frontier Airlines...|        685|\n",
      "|Hawaiian Airlines...|        342|\n",
      "|     JetBlue Airways|      54635|\n",
      "|  Mesa Airlines Inc.|        601|\n",
      "|SkyWest Airlines ...|         32|\n",
      "|Southwest Airline...|      12275|\n",
      "|     US Airways Inc.|      20536|\n",
      "|United Air Lines ...|      58665|\n",
      "|      Virgin America|       5162|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mflightsByCarrier\u001b[39m: \u001b[32mDataFrame\u001b[39m = [name: string, num_flights: bigint]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightsByCarrier = sql(\"\"\"\n",
    "select \n",
    "  name, sum(1) as num_flights \n",
    "from \n",
    "  (\n",
    "    select \n",
    "      airlines.name, flights.* \n",
    "    from \n",
    "      nycflights.airlines \n",
    "    inner join \n",
    "      nycflights.flights \n",
    "    on \n",
    "      airlines.carrier = flights.carrier\n",
    "  ) a\n",
    "group by \n",
    "  name \n",
    "order by \n",
    "  name\n",
    "\"\"\")\n",
    "\n",
    "flightsByCarrier.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV\n",
    "\n",
    "We can read a variety of external formats via the `SparkContext`, and the pattern is largely the same from format to format.  In this case, we read the famous iris dataset which has been saved locally as a csv file as `/data/csv/iris.csv`.  To import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36miris\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Sepal.Length: double, Sepal.Width: double ... 3 more fields]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iris = spark\n",
    "  .read\n",
    "  .format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"delimiter\", \",\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"/data/csv/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|Sepal.Length|Sepal.Width|Petal.Length|Petal.Width|Species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.limit(5).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, if we are working in a 'Big Data' environment, we might expect that our file is saved in Hadoop, rather than locally on disk.  For example, we could copy our file to the Hadoop filesystem by running the following:\n",
    "\n",
    "```bash\n",
    "$ hadoop hdfs -mkdir /data\n",
    "$ hadoop hdfs -mkdir /data/csv\n",
    "$ hadoop hdfs -put /data/csv/iris.csv /data/csv/\n",
    "```\n",
    "\n",
    "The Hadoop filesystem in our case is `hdfs://localhost:9000` (configured in `/opt/hadoop/conf/core-site.xml` via the `fs.defaultFS` property), and in this case we would just change the import as follows:\n",
    "\n",
    "```scala\n",
    "val iris = spark\n",
    "  .read\n",
    "  .format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"delimiter\", \",\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"hdfs://localhost:9000/user/root/iris.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet and Optimized Row Columnar (ORC)\n",
    "\n",
    "Parquet and ORC are popular columnar formats&ndash;parquet more so for Spark, and ORC more so for Hive.  Because data is stored in columns, compression algorithms appropriate for specific columns can be applied, and so the formats generally have good to excellent compression performance.  As an example, the airlines database used above is 53.8MB on disk when stored as CSV, but 7.5MB and 6.0MB when stored as ORC and parquet, respectively.  They also tend to perform very well in read applications like grouped aggregated, though don't fare as well in write applications.  Either way, we could repreduce the example above where we calculated the number of flights by airline as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mairlines\u001b[39m: \u001b[32mDataFrame\u001b[39m = [carrier: string, name: string]\n",
       "\u001b[36mflights\u001b[39m: \u001b[32mDataFrame\u001b[39m = [year: int, month: int ... 17 more fields]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val airlines = spark\n",
    "  .read\n",
    "  .format(\"parquet\")\n",
    "  .load(\"/data/parquet/nycflights/airlines/\")\n",
    "\n",
    "val flights = spark\n",
    "  .read\n",
    "  .format(\"orc\")\n",
    "  .load(\"/data/orc/nycflights/flights/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                name|num_flights|\n",
      "+--------------------+-----------+\n",
      "|AirTran Airways C...|       3260|\n",
      "|Alaska Airlines Inc.|        714|\n",
      "|American Airlines...|      32729|\n",
      "|Delta Air Lines Inc.|      48110|\n",
      "|   Endeavor Air Inc.|      18460|\n",
      "|           Envoy Air|      26397|\n",
      "|ExpressJet Airlin...|      54173|\n",
      "|Frontier Airlines...|        685|\n",
      "|Hawaiian Airlines...|        342|\n",
      "|     JetBlue Airways|      54635|\n",
      "|  Mesa Airlines Inc.|        601|\n",
      "|SkyWest Airlines ...|         32|\n",
      "|Southwest Airline...|      12275|\n",
      "|     US Airways Inc.|      20536|\n",
      "|United Air Lines ...|      58665|\n",
      "|      Virgin America|       5162|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "airlines.as(\"airlines\")\n",
    "  .join(flights.as(\"flights\"), col(\"airlines.carrier\") === col(\"flights.carrier\"), \"inner\")\n",
    "  .groupBy(\"name\")\n",
    "  .count\n",
    "  .withColumnRenamed(\"count\", \"num_flights\")\n",
    "  .orderBy(\"name\")\n",
    "  .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtimeit\u001b[39m"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def timeit[T](block: =>T): (T, Double) = {\n",
    "  val startTime = System.currentTimeMillis()\n",
    "  val res: T = block\n",
    "  (res, System.currentTimeMillis() - startTime)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.11)",
   "language": "scala",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
